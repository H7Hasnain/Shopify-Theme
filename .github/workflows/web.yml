name: Scrape Website

on:
  repository_dispatch:
    types: [scrape-website]
  workflow_dispatch:
    inputs:
      url:
        description: 'Website URL to scrape'
        required: true
        type: string

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          
      - name: Install Puppeteer
        run: npm install puppeteer
        
      - name: Get URL from trigger
        id: get-url
        run: |
          if [ "${{ github.event_name }}" == "repository_dispatch" ]; then
            URL="${{ github.event.client_payload.url }}"
          else
            URL="${{ inputs.url }}"
          fi
          echo "url=$URL" >> $GITHUB_OUTPUT
          echo "Scraping: $URL"
          
      - name: Run scraper
        run: |
          node scraper.js "${{ steps.get-url.outputs.url }}" > output.html 2> scraper.log
          echo "âœ… Scraping completed"
          cat scraper.log || true
          
      - name: Verify output exists
        run: |
          if [ ! -f output.html ]; then
            echo "âŒ Error: output.html was not created"
            exit 1
          fi
          
          SIZE=$(stat -f%z output.html 2>/dev/null || stat -c%s output.html)
          echo "ðŸ“„ Output file size: ${SIZE} bytes"
          
          if [ ${SIZE} -lt 50 ]; then
            echo "âš ï¸  Warning: File seems too small"
            cat output.html
            exit 1
          fi
          
      - name: Save to repository
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          mkdir -p scraped-results
          TIMESTAMP=$(date +%s)
          FILENAME="scraped_${TIMESTAMP}.html"
          
          cp output.html "scraped-results/${FILENAME}"
          echo "ðŸ’¾ Saved as: ${FILENAME}"
          
          git add scraped-results/
          git commit -m "Add scraped content (timestamp: ${TIMESTAMP})"
          git push
          
          echo "âœ… Successfully committed to repository"
          
      - name: Cleanup old files (keep last 10)
        run: |
          cd scraped-results || exit 0
          
          FILE_COUNT=$(ls -1 scraped_*.html 2>/dev/null | wc -l | tr -d ' ')
          echo "ðŸ“Š Total files: ${FILE_COUNT}"
          
          if [ ${FILE_COUNT} -gt 10 ]; then
            echo "ðŸ§¹ Cleaning old files..."
            ls -t scraped_*.html | tail -n +11 | xargs rm -f
            
            cd ..
            git add scraped-results/
            git commit -m "Clean old scraped files (keeping last 10)" || true
            git push || true
          fi
